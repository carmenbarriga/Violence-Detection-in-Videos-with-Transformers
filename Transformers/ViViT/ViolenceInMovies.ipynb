{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPK1g8SrnHvJ3PPHQPpj0Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e14d214c36c4e1dac12f73c8d3b4f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c06343c9c954046acdd01f7b9aab906",
              "IPY_MODEL_8289aa3b014649a481a0dff680530cae",
              "IPY_MODEL_81728179a6d5470b87efd34fdac7bef0"
            ],
            "layout": "IPY_MODEL_76c896c59cc64003a4020d93cf868444"
          }
        },
        "0c06343c9c954046acdd01f7b9aab906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc88cf7cc73040a69f820b61d79520bb",
            "placeholder": "​",
            "style": "IPY_MODEL_f8c35c13940e4f7c86ccfdb75e568126",
            "value": "100%"
          }
        },
        "8289aa3b014649a481a0dff680530cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a072731221904ad6a75b3236b6c97426",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6781de26abb14732a176419843c0cdf6",
            "value": 10
          }
        },
        "81728179a6d5470b87efd34fdac7bef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1148df6143e042338ca3939838b86961",
            "placeholder": "​",
            "style": "IPY_MODEL_b9ee5d7361844fb09aca6d620230e001",
            "value": " 10/10 [04:50&lt;00:00, 28.58s/it]"
          }
        },
        "76c896c59cc64003a4020d93cf868444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc88cf7cc73040a69f820b61d79520bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c35c13940e4f7c86ccfdb75e568126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a072731221904ad6a75b3236b6c97426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6781de26abb14732a176419843c0cdf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1148df6143e042338ca3939838b86961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9ee5d7361844fb09aca6d620230e001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4089f2d3a5a47278ce499cb88de3db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93d0a9e759f6437ab4e8554e9656d61b",
              "IPY_MODEL_6ee9d1e9ef994201a32ddac31cf5cfa3",
              "IPY_MODEL_4c5ad507e6094172bad797df3d8ed83a"
            ],
            "layout": "IPY_MODEL_2efe788f3db84788838b60cfaf4f00e6"
          }
        },
        "93d0a9e759f6437ab4e8554e9656d61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1256e6121a884a818a7688f6b8134ff7",
            "placeholder": "​",
            "style": "IPY_MODEL_c239a42149334a1ba114411aeaac78ad",
            "value": "  0%"
          }
        },
        "6ee9d1e9ef994201a32ddac31cf5cfa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05a308614a0410b9ca3d167b4c66f63",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4a1146eee2b4f2ea2ba6c8c6ab2ac65",
            "value": 0
          }
        },
        "4c5ad507e6094172bad797df3d8ed83a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_440bee8d772b48428abcf2824bcc1505",
            "placeholder": "​",
            "style": "IPY_MODEL_8b415236b0de480ab96e1949120336a9",
            "value": " 0/10 [00:06&lt;?, ?it/s]"
          }
        },
        "2efe788f3db84788838b60cfaf4f00e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1256e6121a884a818a7688f6b8134ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c239a42149334a1ba114411aeaac78ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b05a308614a0410b9ca3d167b4c66f63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a1146eee2b4f2ea2ba6c8c6ab2ac65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "440bee8d772b48428abcf2824bcc1505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b415236b0de480ab96e1949120336a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e5bc7961e504e7cba587f775cb53fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0a10d552e18406d9e7f1324572ac065",
              "IPY_MODEL_a4967eecbf614dd18bb8f5c51b341361",
              "IPY_MODEL_11a7cf6ee6b44ea0b29935fc3d95b32a"
            ],
            "layout": "IPY_MODEL_49bddef112714b379cbece3f58cc30de"
          }
        },
        "d0a10d552e18406d9e7f1324572ac065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e644e99c86a4f569ea86686462814fe",
            "placeholder": "​",
            "style": "IPY_MODEL_60704d02de5e400aa380d684b3e184b2",
            "value": "100%"
          }
        },
        "a4967eecbf614dd18bb8f5c51b341361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_194da583c896466da9c28001c03585db",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d908a6e5ba5407bb75b7ac73059099b",
            "value": 4
          }
        },
        "11a7cf6ee6b44ea0b29935fc3d95b32a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e7477c626ee44b4b311fc302350e6cd",
            "placeholder": "​",
            "style": "IPY_MODEL_beb7cddc641243f28b173807d9822101",
            "value": " 4/4 [00:57&lt;00:00, 13.81s/it, loss=0.493]"
          }
        },
        "49bddef112714b379cbece3f58cc30de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e644e99c86a4f569ea86686462814fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60704d02de5e400aa380d684b3e184b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "194da583c896466da9c28001c03585db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d908a6e5ba5407bb75b7ac73059099b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e7477c626ee44b4b311fc302350e6cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beb7cddc641243f28b173807d9822101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carmenbarriga/Violence-Detection-in-Videos-with-Transformers/blob/main/Transformers/ViViT/ViolenceInMovies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Video Vision Transformer (ViViT) for Violence Detection**\n",
        "@InProceedings{arnab2021vivit,\n",
        "  title={ViViT: A Video Vision Transformer},\n",
        "  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\\v{c}}i{\\'c}, Mario and Schmid, Cordelia},\n",
        "  booktitle={International Conference on Computer Vision (ICCV)},\n",
        "  year={2021}\n",
        "}"
      ],
      "metadata": {
        "id": "lYhh0AVH4It2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.- Installation of the necessary libraries**\n",
        "\n",
        "*   **Einops:** Library that allows to perform tensor operations"
      ],
      "metadata": {
        "id": "vVWH7eGl4WNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFkehZRdtzMo",
        "outputId": "f4226446-909d-4735-e357-d65490eb3ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "! pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.- Mount Google Drive**\n",
        "Mount Google Drive to be able to access Google Drive files and directories"
      ],
      "metadata": {
        "id": "X_zyveSN4p6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuKlJ2rKuDV9",
        "outputId": "c2364a3e-7c46-4ba1-cdb5-1f6988794351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.- Import the necessary libraries**"
      ],
      "metadata": {
        "id": "etEBiHsQ4u4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "from skimage.transform import resize\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn import model_selection\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "gW__IzWOuTRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.- Make some initial configurations**\n",
        "The function `seed_everything` is used to set seeds across various libraries and environments in Python to ensure reproducibility of results. Seed 1001 will be used."
      ],
      "metadata": {
        "id": "ECwFobjw44ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "  # Sets the seed for the numpy library's random number generator\n",
        "  np.random.seed(seed)\n",
        "  # Sets the seed for the torch library's random number generator (PyTorch) for both the CPU and GPU\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  # To ensure that calculations performed with the torch library on the GPU are deterministic\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  # Turn off automatic benchmarking and default settings are used to ensure more stable and predictable execution\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(1001)"
      ],
      "metadata": {
        "id": "khaPgb1kunIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Releases the GPU cache used by PyTorch and displays the current Pytorch version"
      ],
      "metadata": {
        "id": "4PlaAwRk4-yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XjS7oH8muzDn",
        "outputId": "2371968e-0a93-4fe4-94f5-3333927dac95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine on which device the PyTorch computations will be executed, either on a GPU (CUDA) or on the CPU"
      ],
      "metadata": {
        "id": "Ayp8NZ2O5ASi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mTf1TV_ou07V",
        "outputId": "a4db4de6-3c0b-4001-d42e-7a7a11b03bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.- Prepare the data**"
      ],
      "metadata": {
        "id": "0bbqYzI55LrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set **Violence in Movies** dataset folder path\n"
      ],
      "metadata": {
        "id": "wZWOS2YI5QNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "violence_in_movies_folder = '/content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/'\n",
        "violence_in_movies_weights_dir = '/content/drive/MyDrive/transformers-for-violence-detection-in-videos/ViViT/Weights/violence_in_movies_best_model_weights.pth'\n",
        "violence_in_movies_dataframes_folder = '/content/drive/MyDrive/transformers-for-violence-detection-in-videos/ViViT/Dataframes/Violence in Movies/'"
      ],
      "metadata": {
        "id": "l2yuA00ru3pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the following information from the data set:\n",
        "\n",
        "\n",
        "*   Total number of videos\n",
        "*   Minimum duration\n",
        "*   Maximum duration\n",
        "*   Minimum frame rate\n",
        "*   Maximum frame rate\n",
        "*   Average number of frames\n",
        "*   Video widths\n",
        "*   Video heights"
      ],
      "metadata": {
        "id": "vD9oAVqU7MQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_database_info(database_folder):\n",
        "  # Variables to store the shortest and longest duration of the videos\n",
        "  minimum_duration = float('inf')\n",
        "  maximum_duration = float('-inf')\n",
        "\n",
        "  # Variables to store the minimum and maximum frame rate\n",
        "  minimum_fps = float('inf')\n",
        "  maximum_fps = float('-inf')\n",
        "\n",
        "  # Variables to store the minimum and maximum number of frames\n",
        "  minimum_frames = float('inf')\n",
        "  maximum_frames = float('-inf')\n",
        "\n",
        "  # Variable to store the total number of frames in all videos\n",
        "  total_frames = 0\n",
        "\n",
        "  widths = {}\n",
        "  heights = {}\n",
        "\n",
        "  videos_counter = 0\n",
        "\n",
        "  # Loop through the folders (classes) of the dataset folder\n",
        "  for folder_name in os.listdir(database_folder):\n",
        "    folder_dir = database_folder + folder_name + '/'\n",
        "    print(f'Folder name: {folder_name}\\nFolder dir: {folder_dir}')\n",
        "    # Loop through videos within the current folder\n",
        "    for file_name in os.listdir(folder_dir):\n",
        "      file_dir = folder_dir + file_name\n",
        "\n",
        "      # Read the video using OpenCV\n",
        "      cap = cv2.VideoCapture(file_dir)\n",
        "\n",
        "      # Get the frame rate per second (FPS)\n",
        "      fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "      # Update the minimum and maximum frame rate\n",
        "      minimum_fps = min(minimum_fps, fps)\n",
        "      maximum_fps = max(maximum_fps, fps)\n",
        "\n",
        "      # Get the number of frames in the video\n",
        "      num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "      # Update the minimum and maximum number of frames\n",
        "      minimum_frames = min(minimum_frames, num_frames)\n",
        "      maximum_frames = max(maximum_frames, num_frames)      \n",
        "\n",
        "      # Update the total number of frames\n",
        "      total_frames += num_frames\n",
        "\n",
        "      # Get duration in seconds\n",
        "      duration = num_frames / fps\n",
        "    \n",
        "      # Update minimum and maximum duration\n",
        "      minimum_duration = min(minimum_duration, duration)\n",
        "      maximum_duration = max(maximum_duration, duration)\n",
        "    \n",
        "      # Get the resolution (width and height) of the video\n",
        "      width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "      height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "      if width not in widths:\n",
        "        widths[width]=[file_dir]\n",
        "      else:\n",
        "        widths[width].append(file_dir)\n",
        "\n",
        "      if height not in heights:\n",
        "        heights[height]=[file_dir]\n",
        "      else:\n",
        "        heights[height].append(file_dir)\n",
        "\n",
        "      # print(f'Archivo: {file_name}')\n",
        "      # print(f'Duración: {duration} segundos')\n",
        "      # print(f'Resolución: {width}x{height}')\n",
        "      # print(f'Tasa de frames por segundo: {fps}')\n",
        "      # print(f'Número de frames: {num_frames}')\n",
        "\n",
        "      # Release the video capture object\n",
        "      cap.release()          \n",
        "\n",
        "      videos_counter +=1\n",
        "  \n",
        "  print(f'Number of videos: {videos_counter}')\n",
        "\n",
        "  # Calculate the average number of frames in the videos\n",
        "  average_frames = total_frames / videos_counter\n",
        "\n",
        "  # Print shortest and longest duration of videos\n",
        "  print(f'Minimum duration: {minimum_duration} seconds')\n",
        "  print(f'Maximum duration: {maximum_duration} seconds')\n",
        "  \n",
        "  # Print the minimum and maximum frame rate of the videos\n",
        "  print(f'Minimum frame rate: {minimum_fps} fps')\n",
        "  print(f'Maximum frame rate: {maximum_fps} fps')\n",
        "\n",
        "  # Print the minimum and maximum number of frames\n",
        "  print(f'Minimum number of frames: {minimum_frames} fps')\n",
        "  print(f'Maximum number of frames: {maximum_frames} fps')  \n",
        "  \n",
        "  # Print the average number of frames in the videos\n",
        "  print(f'Average number of frames: {average_frames}')\n",
        "  \n",
        "  for key, value in widths.items():\n",
        "    print(f\"Width: {key}\")\n",
        "    print(f\"Number of videos: {len(value)}\")\n",
        "    print(\"------------------------\")\n",
        "\n",
        "  for key, value in heights.items():\n",
        "    print(f\"Height: {key}\")\n",
        "    print(f\"Number of videos: {len(value)}\")\n",
        "    print(\"------------------------\")"
      ],
      "metadata": {
        "id": "q3LzkM4RzbUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_database_info(violence_in_movies_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mrb1urI35v0",
        "outputId": "a8a37e04-1a7b-4540-c823-dd08e694c115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder name: Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Violence/\n",
            "Folder name: Non Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Non Violence/\n",
            "Number of videos: 200\n",
            "Minimum duration: 1.6683333333333334 seconds\n",
            "Maximum duration: 2.04 seconds\n",
            "Minimum frame rate: 25.0 fps\n",
            "Maximum frame rate: 29.97002997002997 fps\n",
            "Minimum number of frames: 42 fps\n",
            "Maximum number of frames: 60 fps\n",
            "Average number of frames: 48.955\n",
            "Width: 720\n",
            "Number of videos: 200\n",
            "------------------------\n",
            "Height: 576\n",
            "Number of videos: 73\n",
            "------------------------\n",
            "Height: 480\n",
            "Number of videos: 127\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to check that videos can be opened correctly"
      ],
      "metadata": {
        "id": "u8R7Eh6L5b60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_frames(video_dir, min_frames=40):\n",
        "  # VideoCapture object to open and read the video\n",
        "  video_capture = cv2.VideoCapture(video_dir)\n",
        "  # To check if the VideoCapture object was able to open the video\n",
        "  if video_capture.isOpened():\n",
        "    # To keep track of how many frames have been counted\n",
        "    frames_counter = 0\n",
        "    while frames_counter < min_frames:\n",
        "      # Read the next frame\n",
        "      is_frame_read, frame = video_capture.read()\n",
        "      # Check if there are no more frames available\n",
        "      if frame is None:\n",
        "        print(f\"Something went wrong with '{video_dir}' video\")\n",
        "        return False\n",
        "      frames_counter += 1\n",
        "  else:\n",
        "    print(f\"Can't open '{video_dir}'\")\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "6JCXpu-du8Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the paths where videos are located and their labels"
      ],
      "metadata": {
        "id": "NyC1oR-55nNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_labels(main_dir):\n",
        "  videos = []\n",
        "  labels = []\n",
        "  # Loop through the folders (classes) of the dataset folder\n",
        "  for folder_name in os.listdir(main_dir):\n",
        "    folder_dir = main_dir + folder_name + '/'\n",
        "    print(f'Folder name: {folder_name}\\nFolder dir: {folder_dir}')\n",
        "    # Loop through videos within the current folder\n",
        "    for file_name in os.listdir(folder_dir):\n",
        "      file_dir = folder_dir + file_name\n",
        "      # Check if the video can be opened correctly and has at least 25 frames\n",
        "      if check_frames(file_dir):\n",
        "        video_dir = os.path.join(folder_dir, file_name)\n",
        "        videos.append(video_dir)\n",
        "        # Add the video label according to the folder where it is located\n",
        "        if folder_name == 'Violence':\n",
        "          labels.append(1)\n",
        "        else:\n",
        "          labels.append(0)\n",
        "  return videos, labels"
      ],
      "metadata": {
        "id": "OObzvjrAu-df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos, labels = get_video_labels(violence_in_movies_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snG9FOsHvJBw",
        "outputId": "d131cdb5-3ea7-48e6-c4a8-f7f4260dcfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder name: Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Violence/\n",
            "Folder name: Non Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Non Violence/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Pandas `DataFrame`"
      ],
      "metadata": {
        "id": "lKt_2sqQ5uO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(data={\"file\": videos, \"label\": labels})\n",
        "data_rows = data.head()\n",
        "data_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HVL-S4b4v9S6",
        "outputId": "9988d888-e354-4379-9d1b-c58a0e2f07d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                file  label\n",
              "0  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "1  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "2  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "3  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "4  /content/drive/MyDrive/transformers-for-violen...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5bd5501f-af91-4e47-95d4-530cccd668fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bd5501f-af91-4e47-95d4-530cccd668fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5bd5501f-af91-4e47-95d4-530cccd668fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5bd5501f-af91-4e47-95d4-530cccd668fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data_rows.iterrows():\n",
        "  file = row['file']\n",
        "  label = row['label']\n",
        "  print(f\"File: {file}, Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDhG_UetBhOO",
        "outputId": "f15b3143-77cf-4e5e-a41d-4761a4afae54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Violence/newfi8.avi, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Violence/newfi5.avi, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Violence/newfi9.avi, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Violence/newfi6.avi, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Violence in Movies/Violence/newfi4.avi, Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data for training and testing:\n",
        "*   80% train\n",
        "*   20% test"
      ],
      "metadata": {
        "id": "jiJx5fwU51Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = model_selection.train_test_split(\n",
        "  data, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "3CLyAV3pvV0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show train data information"
      ],
      "metadata": {
        "id": "-D6LxmKe56dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train data shape: ', train_data.shape)\n",
        "print('Number of violence videos in train data: ', train_data['label'].value_counts()[1])\n",
        "print('Number of non violence videos in train data: ', train_data['label'].value_counts()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK5o3GMKvWpJ",
        "outputId": "f557d48e-5a66-49f7-d285-b99af2cdfbd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  (160, 2)\n",
            "Number of violence videos in train data:  79\n",
            "Number of non violence videos in train data:  81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save train dataframe in a csv file"
      ],
      "metadata": {
        "id": "jjCL4B1eEB9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv(violence_in_movies_dataframes_folder + \"train.csv\", index=False)"
      ],
      "metadata": {
        "id": "mO04vdXiCCVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show test data information"
      ],
      "metadata": {
        "id": "2OEJ5XDS58d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test data shape: ', test_data.shape)\n",
        "print('Number of violence videos in test data: ', test_data['label'].value_counts()[1])\n",
        "print('Number of non violence videos in test data: ', test_data['label'].value_counts()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfgXQwFQvZw_",
        "outputId": "b99521be-ad3b-4e93-f262-a00f988b5331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data shape:  (40, 2)\n",
            "Number of violence videos in test data:  21\n",
            "Number of non violence videos in test data:  19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save test dataframe in a csv file"
      ],
      "metadata": {
        "id": "4j8UdHb5EJ0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv(violence_in_movies_dataframes_folder + \"test.csv\", index=False)"
      ],
      "metadata": {
        "id": "9ogPAYGND8o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining some video properties"
      ],
      "metadata": {
        "id": "l3eCv8hT7ao5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_steps = 32   # Number of frames of each video\n",
        "color_channels = 3  # Number of color channels\n",
        "height = 128  # Height of each frame\n",
        "width = 128   # Width of each frame"
      ],
      "metadata": {
        "id": "FZI_eYp9wKPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to perform the preprocessing of the videos. Videos that contain a greater number of frames than the amount passed to the class will be cut. The videos that contain less than the average amount will be completed with zeros until reaching the average."
      ],
      "metadata": {
        "id": "Jznc7U65F7-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def capture(filename, time_steps, color_channels, height, width):\n",
        "  # Create an array to store the video frames after being processed\n",
        "  frames = np.zeros((time_steps, color_channels, height, width), dtype=float)\n",
        "  # VideoCapture object to open and read the video\n",
        "  video_capture = cv2.VideoCapture(filename)\n",
        "  # To check if the VideoCapture object was able to open the video\n",
        "  if video_capture.isOpened():\n",
        "    # To keep track of how many frames have been stored in the frames array\n",
        "    frames_counter = 0\n",
        "    while frames_counter < time_steps:\n",
        "      # Read the next frame\n",
        "      is_frame_read, frame = video_capture.read()\n",
        "      # Check if there are no more frames available\n",
        "      if not is_frame_read:\n",
        "        break\n",
        "      # Resize the original frame to the specified dimensions (height, width, color_channels) keeping its original aspect ratio\n",
        "      frame = resize(frame, (height, width, color_channels))\n",
        "      # To add an extra dimension (1, height, width, color_channels)\n",
        "      frame = np.expand_dims(frame, axis=0)\n",
        "      # Moves axis -1 (last axis) to index 1 (1, color_channels, height, width)\n",
        "      frame = np.moveaxis(frame, -1, 1)\n",
        "      # Normalization of the pixel values of the frame (if necessary)\n",
        "      if np.max(frame) > 1:\n",
        "        frame = frame / 255.0\n",
        "      # Store the processed frame in the corresponding position within the frames array\n",
        "      frames[frames_counter][:] = frame\n",
        "      frames_counter += 1\n",
        "\n",
        "    del frame\n",
        "    del is_frame_read\n",
        "  frames = np.moveaxis(frames, 1, 0)  # [channels, frames, height, width]\n",
        "\n",
        "  return frames\n",
        "\n",
        "\n",
        "class TaskDataset(Dataset):\n",
        "  def __init__(self, data, time_steps=40, color_channels=3, height=256, width=256):\n",
        "    # data is a pandas dataframe that contains the paths to the video files with their labels\n",
        "    self.data_locations = data\n",
        "    self.time_steps, self.color_channels, self.height, self.width = time_steps, color_channels, height, width\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_locations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    # To process the video and get its frames\n",
        "    video = capture(self.data_locations.iloc[idx, 0], self.time_steps, self.color_channels, self.height, self.width)\n",
        "    # Dictionary containing the processed video, its corresponding label and its path\n",
        "    sample = {\n",
        "      'video': torch.from_numpy(video),\n",
        "      'label': torch.from_numpy(np.asarray(self.data_locations.iloc[idx, 1])),\n",
        "      'path': self.data_locations.iloc[idx, 0]\n",
        "    }\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "75w19Jm-AQ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the training data to the TaskDataset class"
      ],
      "metadata": {
        "id": "Ovilu1997v2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TaskDataset(\n",
        "  data=train_data, time_steps=time_steps, color_channels=color_channels, height=height, width=width\n",
        ")"
      ],
      "metadata": {
        "id": "CeLQOlH07V3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the test data to the TaskDataset class"
      ],
      "metadata": {
        "id": "D9RVUd2N79qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TaskDataset(\n",
        "  data=test_data, time_steps=time_steps, color_channels=color_channels, height=height, width=width\n",
        ")"
      ],
      "metadata": {
        "id": "gg4SNZEywNnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the train batch size"
      ],
      "metadata": {
        "id": "XcX1qv758N3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "dLvYOBTe6sPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a `DataLoader` to load data in batches during training"
      ],
      "metadata": {
        "id": "1oUYJ6bb8VYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "  dataset=train_dataset,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  pin_memory=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0,\n",
        "  shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "PUGGUKj4wrMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a `DataLoader` to load data in batches during test"
      ],
      "metadata": {
        "id": "wofB1R0Z8ubr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_BATCH_SIZE = 10"
      ],
      "metadata": {
        "id": "aUlN-R7zrqmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "  dataset=test_dataset,\n",
        "  batch_size=TEST_BATCH_SIZE,\n",
        "  pin_memory=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0,\n",
        "  shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "-b6w6Xma8Hmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting the `DataLoaders` in the `dataloaders` dictionary and their sizes in the `dataset_sizes` dictionary"
      ],
      "metadata": {
        "id": "mXYn9dDH9_SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {'train': train_loader, 'test': test_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'test': len(test_dataset)}\n",
        "print(dataloaders)\n",
        "print(dataset_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxa06A5q8KU4",
        "outputId": "c44db6c3-257d-4883-a904-42e35fda8014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': <torch.utils.data.dataloader.DataLoader object at 0x7f11e0c84e50>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f11e0c861d0>}\n",
            "{'train': 160, 'test': 40}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To realease the memory because `data`, `train_data` and `test_data` are no longer needed"
      ],
      "metadata": {
        "id": "yk6RslET-jZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del data\n",
        "del train_data\n",
        "del test_data"
      ],
      "metadata": {
        "id": "m7keMujw1--3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.- ViViT**"
      ],
      "metadata": {
        "id": "LA8k9CNlvcZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "  def __init__(self, dimension, fn):\n",
        "    super(PreNorm, self).__init__()\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    return self.fn(self.norm(x), **kwargs)"
      ],
      "metadata": {
        "id": "PEZKiOBb801C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, dimension, heads=8, head_dimension=64, dropout=0.):\n",
        "    super(Attention, self).__init__()\n",
        "    inner_dim = head_dimension * heads\n",
        "    project_out = not (heads == 1 and head_dimension == dimension)\n",
        "\n",
        "    self.heads = heads\n",
        "    self.scale = head_dimension ** -0.5\n",
        "\n",
        "    self.attend = nn.Softmax(dim=-1)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.to_qkv = nn.Linear(dimension, inner_dim * 3, bias=False)\n",
        "\n",
        "    self.to_out = nn.Sequential(\n",
        "      nn.Linear(inner_dim, dimension),\n",
        "      nn.Dropout(dropout)\n",
        "    ) if project_out else nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "    q, k, v = map(lambda t: rearrange(\n",
        "      t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "    dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "    attn = self.attend(dots)\n",
        "    attn = self.dropout(attn)\n",
        "\n",
        "    out = torch.matmul(attn, v)\n",
        "    out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "    return self.to_out(out)"
      ],
      "metadata": {
        "id": "a_l4aDxZ86wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dimension, hidden_dimension, dropout=0.):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "      nn.Linear(dimension, hidden_dimension),\n",
        "      nn.GELU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(hidden_dimension, dimension),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)"
      ],
      "metadata": {
        "id": "AZv_9UsK8_LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, dimension, layers, heads, head_dimension, mlp_dimension, dropout=0.):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for _ in range(layers):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "        PreNorm(dimension, Attention(dimension, heads=heads,\n",
        "          head_dimension=head_dimension, dropout=dropout)),\n",
        "        PreNorm(dimension, FeedForward(\n",
        "          dimension, mlp_dimension, dropout=dropout))\n",
        "      ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x) + x\n",
        "      x = ff(x) + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "6hQVG1M-9BmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViViT(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    height,\n",
        "    width,\n",
        "    frames,\n",
        "    patch_height,\n",
        "    patch_width,\n",
        "    patch_frame,\n",
        "    number_classes,\n",
        "    dimension,\n",
        "    layers=4,\n",
        "    heads=3,\n",
        "    in_channels=3,\n",
        "    head_dimension=64,\n",
        "    dropout=0.,\n",
        "    embedding_dropout=0.,\n",
        "    mlp_dimension=4\n",
        "  ):\n",
        "    super(ViViT, self).__init__()\n",
        "\n",
        "    assert height % patch_height == 0 and width % patch_width == 0, 'Image dimensions must be divisible by the patch size'\n",
        "    assert frames % patch_frame == 0, 'Frames must be divisible by frame patch size'\n",
        "\n",
        "    number_image_patches = (height // patch_height) * \\\n",
        "      (width // patch_width)\n",
        "    number_frame_patches = (frames // patch_frame)\n",
        "\n",
        "    patch_dimension = in_channels * patch_height * patch_width * patch_frame\n",
        "\n",
        "    self.patch_embedding = nn.Sequential(\n",
        "      Rearrange('b c (f pf) (h p1) (w p2) -> b f (h w) (p1 p2 pf c)',\n",
        "        p1=patch_height, p2=patch_width, pf=patch_frame),\n",
        "      nn.LayerNorm(patch_dimension),\n",
        "      nn.Linear(patch_dimension, dimension),\n",
        "      nn.LayerNorm(dimension)\n",
        "    )\n",
        "\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(\n",
        "      1, number_frame_patches, number_image_patches, dimension))\n",
        "    self.dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "    self.spatial_cls_token = nn.Parameter(torch.randn(1, 1, dimension))\n",
        "    self.spatial_transformer = Transformer(\n",
        "      dimension, layers, heads, head_dimension, mlp_dimension, dropout)\n",
        "\n",
        "    self.temporal_cls_token = nn.Parameter(torch.randn(1, 1, dimension))\n",
        "    self.temporal_transformer = Transformer(\n",
        "      dimension, layers, heads, head_dimension, mlp_dimension, dropout)\n",
        "\n",
        "    self.to_latent = nn.Identity()\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "      nn.LayerNorm(dimension),\n",
        "      nn.Linear(dimension, number_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embedding(x)\n",
        "    b, f, n, _ = x.shape\n",
        "\n",
        "    x = x + self.pos_embedding[:, :f, :n]\n",
        "\n",
        "    spatial_cls_tokens = repeat(\n",
        "      self.spatial_cls_token, '1 1 d -> b f 1 d', b=b, f=f)\n",
        "    x = torch.cat((spatial_cls_tokens, x), dim=2)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = rearrange(x, 'b f n d -> (b f) n d')\n",
        "\n",
        "    x = self.spatial_transformer(x)\n",
        "    x = rearrange(x, '(b f) n d -> b f n d', b=b)\n",
        "    x = x[:, :, 0]\n",
        "\n",
        "    temporal_cls_tokens = repeat(\n",
        "      self.temporal_cls_token, '1 1 d-> b 1 d', b=b)\n",
        "    x = torch.cat((temporal_cls_tokens, x), dim=1)\n",
        "\n",
        "    x = self.temporal_transformer(x)\n",
        "    x = x[:, 0]\n",
        "\n",
        "    x = self.to_latent(x)\n",
        "\n",
        "    return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "N3x3vPPH9FsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.- Training**"
      ],
      "metadata": {
        "id": "Wcl6fWQO-swI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, device='cuda', num_epochs=7):\n",
        "  model.to(device)\n",
        "\n",
        "  # Start the training time\n",
        "  since = time.time()\n",
        "\n",
        "  # Save the best loss value during model training\n",
        "  best_loss = float('inf')\n",
        "\n",
        "  # Create a copy of the current model weights\n",
        "  best_model_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions_counter = 0\n",
        "\n",
        "    # To create a progress bar to iterate over the 'train' dataloader using the tqdm library\n",
        "    progress_bar = tqdm(dataloaders['train'], total=int(len(dataloaders['train'])))\n",
        "\n",
        "    for batch, sample in enumerate(progress_bar):\n",
        "      # Get the videos and labels and move them to the corresponding device memory\n",
        "      inputs = sample['video'].to(device, dtype=torch.float)  # [batch_size, time_steps, color_channels, height, width]\n",
        "      labels = sample['label'].view(sample['label'].shape[0], 1).to(device, dtype=torch.float)  # [batch_size] -> [batch_size, 1]\n",
        "\n",
        "      # To clean up the accumulated gradients and ensure that the gradients are calculated correctly \n",
        "      # for the current batch during backpropagation and updating of the weights\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Get the outputs predicted by the model\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Calculate the loss with the function specified in the criterion variable\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Computes the gradients of all model parameters with respect to the loss function\n",
        "      loss.backward()\n",
        "\n",
        "      # Update model parameters based on gradients computed during backpropagation\n",
        "      optimizer.step()\n",
        "\n",
        "      # To get the total loss of the current batch:\n",
        "      #   - loss.item() is the scalar value of the current batch loss\n",
        "      #   - inputs.size(0) gets the batch size\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "      # Apply a sigmoid activation function to the outputs to obtain the predictions\n",
        "      # and round the predictions to be binary (0 or 1)\n",
        "      predictions = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "      # Adds the number of correct predictions in the current batch to the accumulated correct predictions counter\n",
        "      correct_predictions_counter += torch.sum(predictions == labels.data)\n",
        "\n",
        "    # Calculates the average loss for each epoch\n",
        "    epoch_loss = running_loss / dataset_sizes['train']\n",
        "    # Calculates the accuracy for each epoch\n",
        "    epoch_accuracy = correct_predictions_counter.double() / dataset_sizes['train']\n",
        "    print('Train Loss: {:.4f} Accuracy: {:.4f}'.format(epoch_loss, epoch_accuracy))\n",
        "\n",
        "    # Updates the state of the optimizer based on the loss obtained in each training epoch\n",
        "    scheduler.step(epoch_loss)\n",
        "\n",
        "    # Stores the model weights that correspond to the best loss achieved so far\n",
        "    if epoch_loss < best_loss:\n",
        "      best_loss = epoch_loss\n",
        "      best_model_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  # End the training time\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "  # The model is loaded with the weights corresponding to the best saved model\n",
        "  model.load_state_dict(best_model_weights)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "arZEXHecxGtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model"
      ],
      "metadata": {
        "id": "N4QmYaLnD3sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViViT(\n",
        "  height=height,\n",
        "  width=width,\n",
        "  frames=time_steps,\n",
        "  patch_height=8,\n",
        "  patch_width=8,\n",
        "  patch_frame=8,\n",
        "  number_classes=1,\n",
        "  dimension=128,\n",
        "  layers=8,\n",
        "  heads=8,\n",
        "  in_channels=3,\n",
        "  head_dimension=64,\n",
        "  dropout=0.,\n",
        "  embedding_dropout=0.,\n",
        "  mlp_dimension=4\n",
        ")\n",
        "model"
      ],
      "metadata": {
        "id": "UdCFypQjxL0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fcf0a67-d707-4cb3-f925-46e599e9629d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViViT(\n",
              "  (patch_embedding): Sequential(\n",
              "    (0): Rearrange('b c (f pf) (h p1) (w p2) -> b f (h w) (p1 p2 pf c)', p1=8, p2=8, pf=8)\n",
              "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): Linear(in_features=1536, out_features=128, bias=True)\n",
              "    (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (spatial_transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (network): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=4, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=4, out_features=128, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (temporal_transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (network): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=4, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=4, out_features=128, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_latent): Identity()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "model = train_model(model, criterion, optimizer, scheduler, device=device, num_epochs=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "6e14d214c36c4e1dac12f73c8d3b4f5f",
            "0c06343c9c954046acdd01f7b9aab906",
            "8289aa3b014649a481a0dff680530cae",
            "81728179a6d5470b87efd34fdac7bef0",
            "76c896c59cc64003a4020d93cf868444",
            "cc88cf7cc73040a69f820b61d79520bb",
            "f8c35c13940e4f7c86ccfdb75e568126",
            "a072731221904ad6a75b3236b6c97426",
            "6781de26abb14732a176419843c0cdf6",
            "1148df6143e042338ca3939838b86961",
            "b9ee5d7361844fb09aca6d620230e001",
            "a4089f2d3a5a47278ce499cb88de3db2",
            "93d0a9e759f6437ab4e8554e9656d61b",
            "6ee9d1e9ef994201a32ddac31cf5cfa3",
            "4c5ad507e6094172bad797df3d8ed83a",
            "2efe788f3db84788838b60cfaf4f00e6",
            "1256e6121a884a818a7688f6b8134ff7",
            "c239a42149334a1ba114411aeaac78ad",
            "b05a308614a0410b9ca3d167b4c66f63",
            "b4a1146eee2b4f2ea2ba6c8c6ab2ac65",
            "440bee8d772b48428abcf2824bcc1505",
            "8b415236b0de480ab96e1949120336a9"
          ]
        },
        "id": "j4dZONWiAzSc",
        "outputId": "c6b057f8-aa86-46d1-c88c-64ddadfa715e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "----------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e14d214c36c4e1dac12f73c8d3b4f5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6722 Accuracy: 0.5938\n",
            "Epoch 2/7\n",
            "----------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4089f2d3a5a47278ce499cb88de3db2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2bbd9f451bb1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-f5a91f48c8ff>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0;31m# Get the videos and labels and move them to the corresponding device memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch_size, time_steps, color_channels, height, width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-1db0e3b747fc>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# To process the video and get its frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_locations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Dictionary containing the processed video, its corresponding label and its path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     sample = {\n",
            "\u001b[0;32m<ipython-input-21-1db0e3b747fc>\u001b[0m in \u001b[0;36mcapture\u001b[0;34m(filename, time_steps, color_channels, height, width)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# Resize the original frame to the specified dimensions (height, width, color_channels) keeping its original aspect ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0;31m# To add an extra dimension (1, height, width, color_channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 warn(\"Anti-aliasing standard deviation greater than zero but \"\n\u001b[1;32m    180\u001b[0m                      \"not down-sampling along all axes\")\n\u001b[0;32m--> 181\u001b[0;31m         image = ndi.gaussian_filter(image, anti_aliasing_sigma,\n\u001b[0m\u001b[1;32m    182\u001b[0m                                     cval=cval, mode=ndi_mode)\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_filters.py\u001b[0m in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             gaussian_filter1d(input, sigma, axis, order, output,\n\u001b[0m\u001b[1;32m    369\u001b[0m                               mode, cval, truncate, radius=radius)\n\u001b[1;32m    370\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_filters.py\u001b[0m in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate, radius)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# Since we are calling correlate, not convolve, revert the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gaussian_kernel1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelate1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_filters.py\u001b[0m in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m    132\u001b[0m                          '(len(weights)-1) // 2')\n\u001b[1;32m    133\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _nd_image.correlate1d(input, weights, axis, output, mode, cval,\n\u001b[0m\u001b[1;32m    135\u001b[0m                           origin)\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.- Test**"
      ],
      "metadata": {
        "id": "GVbyk4L-o5FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, criterion, device='cuda'):\n",
        "  model.to(device)\n",
        "\n",
        "  # To start the evaluation time\n",
        "  since = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  correct_predictions_counter = 0\n",
        "\n",
        "  pred_vs_real = {}\n",
        "  pred_vs_real['path']= []\n",
        "  pred_vs_real['label']= []  \n",
        "  pred_vs_real['prediction']= []\n",
        "\n",
        "  # To create a progress bar to iterate over the 'test' dataloader using the tqdm library\n",
        "  progress_bar = tqdm(dataloaders['test'], total=int(len(dataloaders['test'])))\n",
        "\n",
        "  processed_batch_counter = 0\n",
        "  for batch, sample in enumerate(progress_bar):\n",
        "    # Get the videos and labels and move them to the corresponding device memory\n",
        "    inputs = sample['video'].to(device , dtype=torch.float)\n",
        "    labels = sample['label'].view(sample['label'].shape[0], 1).to(device, dtype=torch.float)\n",
        "    paths = sample['path']\n",
        "\n",
        "    # Get the outputs predicted by the model\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Apply a sigmoid activation function to the outputs to obtain the predictions\n",
        "    # and round the predictions to be binary (0 or 1)\n",
        "    predictions = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "    # Add the predictions and labels to the dictionary pred_vs_real\n",
        "    # converted to a numpy array and move them to CPU memory\n",
        "    pred_vs_real['prediction'].extend(predictions.cpu().detach().numpy().flatten())\n",
        "    pred_vs_real['label'].extend(labels.cpu().detach().numpy().flatten())\n",
        "    pred_vs_real['path'].extend(list(paths))\n",
        "\n",
        "    # Calculate the loss with the function specified in the criterion variable\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # To get the total loss of the current batch:\n",
        "    #   - loss.item() is the scalar value of the current batch loss\n",
        "    #   - inputs.size(0) gets the batch size\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    # Adds the number of correct predictions in the current batch to the accumulated correct predictions counter\n",
        "    correct_predictions_counter += torch.sum(predictions == labels.data)\n",
        "\n",
        "    # Updates the progress message in the progress_bar iterator showing the average loss\n",
        "    # To do this, divide the accumulated loss by the total number of samples processed so far\n",
        "    processed_batch_counter += 1\n",
        "    progress_bar.set_postfix(loss=(running_loss / (processed_batch_counter * dataloaders['test'].batch_size)))\n",
        "\n",
        "  final_loss = running_loss / dataset_sizes['test']\n",
        "  accuracy = correct_predictions_counter.double() / dataset_sizes['test']\n",
        "  precision = precision_score(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  recall = recall_score(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  f1 = f1_score(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  print('{} Loss: {:.4f} Accuracy: {:.4f} Precision: {:.4f} Recall: {:.4f} F1 Score: {:.4f}'.format('Test', final_loss, accuracy, precision, recall, f1))\n",
        "\n",
        "  # Calculate and print the confusion matrix\n",
        "  confusion = confusion_matrix(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion)\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Testing complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "  return pred_vs_real"
      ],
      "metadata": {
        "id": "AF9ldUTwo1nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_vs_real = test_model(model, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "6e5bc7961e504e7cba587f775cb53fbc",
            "d0a10d552e18406d9e7f1324572ac065",
            "a4967eecbf614dd18bb8f5c51b341361",
            "11a7cf6ee6b44ea0b29935fc3d95b32a",
            "49bddef112714b379cbece3f58cc30de",
            "0e644e99c86a4f569ea86686462814fe",
            "60704d02de5e400aa380d684b3e184b2",
            "194da583c896466da9c28001c03585db",
            "5d908a6e5ba5407bb75b7ac73059099b",
            "3e7477c626ee44b4b311fc302350e6cd",
            "beb7cddc641243f28b173807d9822101"
          ]
        },
        "id": "pG38rmi5pV_m",
        "outputId": "ea62b948-a02f-495a-c7cb-bb02170e5ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e5bc7961e504e7cba587f775cb53fbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.4930 Accuracy: 0.7000 Precision: 0.7647 Recall: 0.6190 F1 Score: 0.6842\n",
            "Confusion Matrix:\n",
            "[[15  4]\n",
            " [ 8 13]]\n",
            "Testing complete in 0m 57s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model test results in a CSV file"
      ],
      "metadata": {
        "id": "yRLaxI0BTM7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with the data from pred_vs_real\n",
        "pred_vs_real_dataframe = pd.DataFrame({'path': pred_vs_real['path'], 'label': pred_vs_real['label'], 'prediction': pred_vs_real['prediction']})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "pred_vs_real_dataframe.to_csv(violence_in_movies_dataframes_folder + 'results.csv', index=False)"
      ],
      "metadata": {
        "id": "n5xSYQ60rIWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}