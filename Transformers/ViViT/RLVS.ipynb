{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNH0bqzZ7y0JRhmqd+mbhz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a82cbb5bb8fc4d39b08553e5b94015a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_589154f59cc54310a75b859dce4d856c",
              "IPY_MODEL_1f34080f1da64a22bde0789c02f09970",
              "IPY_MODEL_1a57300afe284242a7058f2cea0d96ba"
            ],
            "layout": "IPY_MODEL_4f9a4c82f4614c14900a940aeeb18630"
          }
        },
        "589154f59cc54310a75b859dce4d856c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_631362d2b5524547863889d1fafdca00",
            "placeholder": "​",
            "style": "IPY_MODEL_20b4928b637f4d49b7f3908713eea98d",
            "value": "  0%"
          }
        },
        "1f34080f1da64a22bde0789c02f09970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c50805d2054128b7031dd4d0822a5b",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d834c0fe29c4b4d88d65f36132ee938",
            "value": 0
          }
        },
        "1a57300afe284242a7058f2cea0d96ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fab4709dbc354ef4bb9f5b596e957383",
            "placeholder": "​",
            "style": "IPY_MODEL_f55c9ac11bc7486b8e54c38e1bb7acce",
            "value": " 0/100 [00:44&lt;?, ?it/s]"
          }
        },
        "4f9a4c82f4614c14900a940aeeb18630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "631362d2b5524547863889d1fafdca00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b4928b637f4d49b7f3908713eea98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8c50805d2054128b7031dd4d0822a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d834c0fe29c4b4d88d65f36132ee938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fab4709dbc354ef4bb9f5b596e957383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55c9ac11bc7486b8e54c38e1bb7acce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1bcee788e434b74b5eb4c1d3201d52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3188afc930434f86b7b7cd828649ef3f",
              "IPY_MODEL_95589afca35449ed8e5b2c2c4282fa15",
              "IPY_MODEL_6024c01adb6f46a1b6f4721cea92c5b1"
            ],
            "layout": "IPY_MODEL_e3bbda22a7eb4a6d8a2525680e18b1c9"
          }
        },
        "3188afc930434f86b7b7cd828649ef3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df803846cf59437db37dde53ee437895",
            "placeholder": "​",
            "style": "IPY_MODEL_72ed71477027460b8f3048cf1fc4d6fe",
            "value": "100%"
          }
        },
        "95589afca35449ed8e5b2c2c4282fa15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0005fab6a64494a9680415e23134f12",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_099c7014fcba45948aefb4253e0dda66",
            "value": 40
          }
        },
        "6024c01adb6f46a1b6f4721cea92c5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a302c9f096bd43e5ae717ce2d3caa37b",
            "placeholder": "​",
            "style": "IPY_MODEL_ba4c2769480d4b21b31851684fe2a992",
            "value": " 40/40 [07:40&lt;00:00, 11.35s/it, loss=0.736]"
          }
        },
        "e3bbda22a7eb4a6d8a2525680e18b1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df803846cf59437db37dde53ee437895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ed71477027460b8f3048cf1fc4d6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0005fab6a64494a9680415e23134f12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "099c7014fcba45948aefb4253e0dda66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a302c9f096bd43e5ae717ce2d3caa37b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba4c2769480d4b21b31851684fe2a992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carmenbarriga/Violence-Detection-in-Videos-with-Transformers/blob/main/Transformers/ViViT/RLVS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Video Vision Transformer (ViViT) for Violence Detection**\n",
        "@InProceedings{arnab2021vivit,\n",
        "  title={ViViT: A Video Vision Transformer},\n",
        "  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\\v{c}}i{\\'c}, Mario and Schmid, Cordelia},\n",
        "  booktitle={International Conference on Computer Vision (ICCV)},\n",
        "  year={2021}\n",
        "}"
      ],
      "metadata": {
        "id": "lYhh0AVH4It2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.- Installation of the necessary libraries**\n",
        "\n",
        "*   **Einops:** Library that allows to perform tensor operations"
      ],
      "metadata": {
        "id": "vVWH7eGl4WNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFkehZRdtzMo",
        "outputId": "92c2a210-62bd-41d5-afa5-97dd1b41dbab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "! pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.- Mount Google Drive**\n",
        "Mount Google Drive to be able to access Google Drive files and directories"
      ],
      "metadata": {
        "id": "X_zyveSN4p6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuKlJ2rKuDV9",
        "outputId": "449fe516-b201-4dae-b6cb-5b94354711cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.- Import the necessary libraries**"
      ],
      "metadata": {
        "id": "etEBiHsQ4u4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "from skimage.transform import resize\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn import model_selection\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import lr_scheduler\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "gW__IzWOuTRv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.- Make some initial configurations**\n",
        "The function `seed_everything` is used to set seeds across various libraries and environments in Python to ensure reproducibility of results. Seed 1001 will be used."
      ],
      "metadata": {
        "id": "ECwFobjw44ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "  # Sets the seed for the numpy library's random number generator\n",
        "  np.random.seed(seed)\n",
        "  # Sets the seed for the torch library's random number generator (PyTorch) for both the CPU and GPU\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  # To ensure that calculations performed with the torch library on the GPU are deterministic\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  # Turn off automatic benchmarking and default settings are used to ensure more stable and predictable execution\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(1001)"
      ],
      "metadata": {
        "id": "khaPgb1kunIU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Releases the GPU cache used by PyTorch and displays the current Pytorch version"
      ],
      "metadata": {
        "id": "4PlaAwRk4-yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XjS7oH8muzDn",
        "outputId": "6ba8afc8-6026-4109-cbe5-2fb8ad4f9b7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine on which device the PyTorch computations will be executed, either on a GPU (CUDA) or on the CPU"
      ],
      "metadata": {
        "id": "Ayp8NZ2O5ASi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mTf1TV_ou07V",
        "outputId": "8b4fe4f4-602d-4499-8bdc-d4928ac6a153"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.- Prepare the data**"
      ],
      "metadata": {
        "id": "0bbqYzI55LrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set **Real Life Violence Dataset** dataset folder path\n"
      ],
      "metadata": {
        "id": "wZWOS2YI5QNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rlvs_folder = '/content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/'\n",
        "rlvs_weights_dir = '/content/drive/MyDrive/transformers-for-violence-detection-in-videos/ViViT/Weights/rlvs_best_model_weights.pth'\n",
        "rlvs_dataframes_folder = '/content/drive/MyDrive/transformers-for-violence-detection-in-videos/ViViT/Dataframes/Real Life Violence Dataset/'"
      ],
      "metadata": {
        "id": "l2yuA00ru3pu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the following information from the data set:\n",
        "\n",
        "\n",
        "*   Total number of videos\n",
        "*   Minimum duration\n",
        "*   Maximum duration\n",
        "*   Minimum frame rate\n",
        "*   Maximum frame rate\n",
        "*   Average number of frames\n",
        "*   Video widths\n",
        "*   Video heights"
      ],
      "metadata": {
        "id": "vD9oAVqU7MQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_database_info(database_folder):\n",
        "  # Variables to store the shortest and longest duration of the videos\n",
        "  minimum_duration = float('inf')\n",
        "  maximum_duration = float('-inf')\n",
        "\n",
        "  # Variables to store the minimum and maximum frame rate\n",
        "  minimum_fps = float('inf')\n",
        "  maximum_fps = float('-inf')\n",
        "\n",
        "  # Variables to store the minimum and maximum number of frames\n",
        "  minimum_frames = float('inf')\n",
        "  maximum_frames = float('-inf')\n",
        "\n",
        "  # Variable to store the total number of frames in all videos\n",
        "  total_frames = 0\n",
        "\n",
        "  widths = {}\n",
        "  heights = {}\n",
        "\n",
        "  videos_counter = 0\n",
        "\n",
        "  # Loop through the folders (classes) of the dataset folder\n",
        "  for folder_name in os.listdir(database_folder):\n",
        "    folder_dir = database_folder + folder_name + '/'\n",
        "    print(f'Folder name: {folder_name}\\nFolder dir: {folder_dir}')\n",
        "    # Loop through videos within the current folder\n",
        "    for file_name in os.listdir(folder_dir):\n",
        "      file_dir = folder_dir + file_name\n",
        "\n",
        "      # Read the video using OpenCV\n",
        "      cap = cv2.VideoCapture(file_dir)\n",
        "\n",
        "      # Get the frame rate per second (FPS)\n",
        "      fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "      # Update the minimum and maximum frame rate\n",
        "      minimum_fps = min(minimum_fps, fps)\n",
        "      maximum_fps = max(maximum_fps, fps)\n",
        "\n",
        "      # Get the number of frames in the video\n",
        "      num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "      # Update the minimum and maximum number of frames\n",
        "      minimum_frames = min(minimum_frames, num_frames)\n",
        "      maximum_frames = max(maximum_frames, num_frames)      \n",
        "\n",
        "      # Update the total number of frames\n",
        "      total_frames += num_frames\n",
        "\n",
        "      # Get duration in seconds\n",
        "      duration = num_frames / fps\n",
        "    \n",
        "      # Update minimum and maximum duration\n",
        "      minimum_duration = min(minimum_duration, duration)\n",
        "      maximum_duration = max(maximum_duration, duration)\n",
        "    \n",
        "      # Get the resolution (width and height) of the video\n",
        "      width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "      height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "      if width not in widths:\n",
        "        widths[width]=[file_dir]\n",
        "      else:\n",
        "        widths[width].append(file_dir)\n",
        "\n",
        "      if height not in heights:\n",
        "        heights[height]=[file_dir]\n",
        "      else:\n",
        "        heights[height].append(file_dir)\n",
        "\n",
        "      # print(f'Archivo: {file_name}')\n",
        "      # print(f'Duración: {duration} segundos')\n",
        "      # print(f'Resolución: {width}x{height}')\n",
        "      # print(f'Tasa de frames por segundo: {fps}')\n",
        "      # print(f'Número de frames: {num_frames}')\n",
        "\n",
        "      # Release the video capture object\n",
        "      cap.release()          \n",
        "\n",
        "      videos_counter +=1\n",
        "  \n",
        "  print(f'Number of videos: {videos_counter}')\n",
        "\n",
        "  # Calculate the average number of frames in the videos\n",
        "  average_frames = total_frames / videos_counter\n",
        "\n",
        "  # Print shortest and longest duration of videos\n",
        "  print(f'Minimum duration: {minimum_duration} seconds')\n",
        "  print(f'Maximum duration: {maximum_duration} seconds')\n",
        "  \n",
        "  # Print the minimum and maximum frame rate of the videos\n",
        "  print(f'Minimum frame rate: {minimum_fps} fps')\n",
        "  print(f'Maximum frame rate: {maximum_fps} fps')\n",
        "\n",
        "  # Print the minimum and maximum number of frames\n",
        "  print(f'Minimum number of frames: {minimum_frames} fps')\n",
        "  print(f'Maximum number of frames: {maximum_frames} fps')  \n",
        "  \n",
        "  # Print the average number of frames in the videos\n",
        "  print(f'Average number of frames: {average_frames}')\n",
        "  \n",
        "  for key, value in widths.items():\n",
        "    print(f\"Width: {key}\")\n",
        "    print(f\"Number of videos: {len(value)}\")\n",
        "    print(\"------------------------\")\n",
        "\n",
        "  for key, value in heights.items():\n",
        "    print(f\"Height: {key}\")\n",
        "    print(f\"Number of videos: {len(value)}\")\n",
        "    print(\"------------------------\")"
      ],
      "metadata": {
        "id": "q3LzkM4RzbUh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_database_info(rlvs_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mrb1urI35v0",
        "outputId": "48de773b-e2c0-498e-9320-1ea41ad4ad4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder name: Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Violence/\n",
            "Folder name: Non Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Non Violence/\n",
            "Number of videos: 2000\n",
            "Minimum duration: 1.0 seconds\n",
            "Maximum duration: 375.73333333333335 seconds\n",
            "Minimum frame rate: 10.5 fps\n",
            "Maximum frame rate: 37.0 fps\n",
            "Minimum number of frames: 29 fps\n",
            "Maximum number of frames: 11272 fps\n",
            "Average number of frames: 143.6845\n",
            "Width: 260\n",
            "Number of videos: 5\n",
            "------------------------\n",
            "Width: 640\n",
            "Number of videos: 201\n",
            "------------------------\n",
            "Width: 1280\n",
            "Number of videos: 309\n",
            "------------------------\n",
            "Width: 1920\n",
            "Number of videos: 21\n",
            "------------------------\n",
            "Width: 204\n",
            "Number of videos: 14\n",
            "------------------------\n",
            "Width: 410\n",
            "Number of videos: 7\n",
            "------------------------\n",
            "Width: 854\n",
            "Number of videos: 37\n",
            "------------------------\n",
            "Width: 210\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 320\n",
            "Number of videos: 18\n",
            "------------------------\n",
            "Width: 576\n",
            "Number of videos: 8\n",
            "------------------------\n",
            "Width: 1070\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 404\n",
            "Number of videos: 7\n",
            "------------------------\n",
            "Width: 286\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 718\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 1038\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 414\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 438\n",
            "Number of videos: 8\n",
            "------------------------\n",
            "Width: 402\n",
            "Number of videos: 11\n",
            "------------------------\n",
            "Width: 230\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 408\n",
            "Number of videos: 12\n",
            "------------------------\n",
            "Width: 442\n",
            "Number of videos: 11\n",
            "------------------------\n",
            "Width: 584\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 262\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 428\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Width: 314\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 440\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 202\n",
            "Number of videos: 24\n",
            "------------------------\n",
            "Width: 338\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 398\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Width: 654\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Width: 282\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 728\n",
            "Number of videos: 5\n",
            "------------------------\n",
            "Width: 1080\n",
            "Number of videos: 9\n",
            "------------------------\n",
            "Width: 180\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 360\n",
            "Number of videos: 50\n",
            "------------------------\n",
            "Width: 484\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 368\n",
            "Number of videos: 9\n",
            "------------------------\n",
            "Width: 960\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 400\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Width: 766\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 632\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 406\n",
            "Number of videos: 60\n",
            "------------------------\n",
            "Width: 396\n",
            "Number of videos: 10\n",
            "------------------------\n",
            "Width: 470\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 712\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 430\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 488\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 656\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Width: 592\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 424\n",
            "Number of videos: 11\n",
            "------------------------\n",
            "Width: 224\n",
            "Number of videos: 948\n",
            "------------------------\n",
            "Width: 206\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Width: 452\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 162\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 240\n",
            "Number of videos: 10\n",
            "------------------------\n",
            "Width: 492\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 804\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 724\n",
            "Number of videos: 10\n",
            "------------------------\n",
            "Width: 716\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Width: 650\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Width: 394\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 474\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 416\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 352\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 976\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 608\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 614\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 744\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 822\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 834\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 526\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 644\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 670\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 926\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Width: 720\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Width: 726\n",
            "Number of videos: 15\n",
            "------------------------\n",
            "Width: 714\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Width: 288\n",
            "Number of videos: 14\n",
            "------------------------\n",
            "Width: 136\n",
            "Number of videos: 7\n",
            "------------------------\n",
            "Width: 228\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Width: 198\n",
            "Number of videos: 8\n",
            "------------------------\n",
            "Width: 194\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Width: 292\n",
            "Number of videos: 21\n",
            "------------------------\n",
            "Height: 360\n",
            "Number of videos: 348\n",
            "------------------------\n",
            "Height: 720\n",
            "Number of videos: 387\n",
            "------------------------\n",
            "Height: 1080\n",
            "Number of videos: 22\n",
            "------------------------\n",
            "Height: 652\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 540\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 358\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 240\n",
            "Number of videos: 26\n",
            "------------------------\n",
            "Height: 452\n",
            "Number of videos: 5\n",
            "------------------------\n",
            "Height: 498\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 534\n",
            "Number of videos: 5\n",
            "------------------------\n",
            "Height: 650\n",
            "Number of videos: 10\n",
            "------------------------\n",
            "Height: 484\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 362\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 336\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 260\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 574\n",
            "Number of videos: 9\n",
            "------------------------\n",
            "Height: 570\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Height: 626\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 628\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Height: 1920\n",
            "Number of videos: 9\n",
            "------------------------\n",
            "Height: 548\n",
            "Number of videos: 5\n",
            "------------------------\n",
            "Height: 530\n",
            "Number of videos: 7\n",
            "------------------------\n",
            "Height: 602\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 542\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 672\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 656\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 224\n",
            "Number of videos: 952\n",
            "------------------------\n",
            "Height: 354\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 352\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Height: 716\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Height: 430\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 434\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 302\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 466\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Height: 406\n",
            "Number of videos: 10\n",
            "------------------------\n",
            "Height: 606\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Height: 712\n",
            "Number of videos: 15\n",
            "------------------------\n",
            "Height: 480\n",
            "Number of videos: 36\n",
            "------------------------\n",
            "Height: 440\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 242\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 350\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 514\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 404\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 510\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 320\n",
            "Number of videos: 10\n",
            "------------------------\n",
            "Height: 546\n",
            "Number of videos: 9\n",
            "------------------------\n",
            "Height: 668\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Height: 482\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 504\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 536\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Height: 664\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 512\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 600\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Height: 608\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 700\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 614\n",
            "Number of videos: 3\n",
            "------------------------\n",
            "Height: 644\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 604\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 714\n",
            "Number of videos: 13\n",
            "------------------------\n",
            "Height: 692\n",
            "Number of videos: 2\n",
            "------------------------\n",
            "Height: 472\n",
            "Number of videos: 1\n",
            "------------------------\n",
            "Height: 670\n",
            "Number of videos: 10\n",
            "------------------------\n",
            "Height: 620\n",
            "Number of videos: 4\n",
            "------------------------\n",
            "Height: 296\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Height: 298\n",
            "Number of videos: 6\n",
            "------------------------\n",
            "Height: 272\n",
            "Number of videos: 8\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to check that videos can be opened correctly"
      ],
      "metadata": {
        "id": "u8R7Eh6L5b60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_frames(video_dir, min_frames=29):\n",
        "  # VideoCapture object to open and read the video\n",
        "  video_capture = cv2.VideoCapture(video_dir)\n",
        "  # To check if the VideoCapture object was able to open the video\n",
        "  if video_capture.isOpened():\n",
        "    # To keep track of how many frames have been counted\n",
        "    frames_counter = 0\n",
        "    while frames_counter < min_frames:\n",
        "      # Read the next frame\n",
        "      is_frame_read, frame = video_capture.read()\n",
        "      # Check if there are no more frames available\n",
        "      if frame is None:\n",
        "        print(f\"Something went wrong with '{video_dir}' video\")\n",
        "        return False\n",
        "      frames_counter += 1\n",
        "  else:\n",
        "    print(f\"Can't open '{video_dir}'\")\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "6JCXpu-du8Mf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the paths where videos are located and their labels"
      ],
      "metadata": {
        "id": "NyC1oR-55nNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_labels(main_dir):\n",
        "  videos = []\n",
        "  labels = []\n",
        "  # Loop through the folders (classes) of the dataset folder\n",
        "  for folder_name in os.listdir(main_dir):\n",
        "    folder_dir = main_dir + folder_name + '/'\n",
        "    print(f'Folder name: {folder_name}\\nFolder dir: {folder_dir}')\n",
        "    # Loop through videos within the current folder\n",
        "    for file_name in os.listdir(folder_dir):\n",
        "      file_dir = folder_dir + file_name\n",
        "      # Check if the video can be opened correctly and has at least 25 frames\n",
        "      if check_frames(file_dir):\n",
        "        video_dir = os.path.join(folder_dir, file_name)\n",
        "        videos.append(video_dir)\n",
        "        # Add the video label according to the folder where it is located\n",
        "        if folder_name == 'Violence':\n",
        "          labels.append(1)\n",
        "        else:\n",
        "          labels.append(0)\n",
        "  return videos, labels"
      ],
      "metadata": {
        "id": "OObzvjrAu-df"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos, labels = get_video_labels(rlvs_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snG9FOsHvJBw",
        "outputId": "41b0805e-c1ae-4bc3-ed79-80ff51b5fac6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder name: Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Violence/\n",
            "Folder name: Non Violence\n",
            "Folder dir: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Non Violence/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Pandas `DataFrame`"
      ],
      "metadata": {
        "id": "lKt_2sqQ5uO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(data={\"file\": videos, \"label\": labels})\n",
        "data_rows = data.head()\n",
        "data_rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HVL-S4b4v9S6",
        "outputId": "99e59a18-0b2e-4ad9-c9fc-3b75548d3539"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                file  label\n",
              "0  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "1  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "2  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "3  /content/drive/MyDrive/transformers-for-violen...      1\n",
              "4  /content/drive/MyDrive/transformers-for-violen...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74a781a7-46c8-4f50-830c-e532300aaa08\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/transformers-for-violen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74a781a7-46c8-4f50-830c-e532300aaa08')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74a781a7-46c8-4f50-830c-e532300aaa08 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74a781a7-46c8-4f50-830c-e532300aaa08');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data_rows.iterrows():\n",
        "  file = row['file']\n",
        "  label = row['label']\n",
        "  print(f\"File: {file}, Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDhG_UetBhOO",
        "outputId": "bb43b8fc-0f83-44d0-c0da-12eb593c693e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Violence/V_1000.mp4, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Violence/V_107.mp4, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Violence/V_114.mp4, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Violence/V_11.mp4, Label: 1\n",
            "File: /content/drive/MyDrive/transformers-for-violence-detection-in-videos/Datasets/Real Life Violence Dataset/Violence/V_103.mp4, Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data for training and testing:\n",
        "*   80% train\n",
        "*   20% test"
      ],
      "metadata": {
        "id": "jiJx5fwU51Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = model_selection.train_test_split(\n",
        "  data, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "3CLyAV3pvV0G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show train data information"
      ],
      "metadata": {
        "id": "-D6LxmKe56dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train data shape: ', train_data.shape)\n",
        "print('Number of violence videos in train data: ', train_data['label'].value_counts()[1])\n",
        "print('Number of non violence videos in train data: ', train_data['label'].value_counts()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK5o3GMKvWpJ",
        "outputId": "3aefa40c-e5cc-45b8-f6dd-2ec0aebfaac5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  (1600, 2)\n",
            "Number of violence videos in train data:  801\n",
            "Number of non violence videos in train data:  799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save train dataframe in a csv file"
      ],
      "metadata": {
        "id": "jjCL4B1eEB9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv(rlvs_dataframes_folder + \"train.csv\", index=False)"
      ],
      "metadata": {
        "id": "mO04vdXiCCVL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show test data information"
      ],
      "metadata": {
        "id": "2OEJ5XDS58d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test data shape: ', test_data.shape)\n",
        "print('Number of violence videos in test data: ', test_data['label'].value_counts()[1])\n",
        "print('Number of non violence videos in test data: ', test_data['label'].value_counts()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfgXQwFQvZw_",
        "outputId": "2e59cb20-e849-4af2-f397-fc8e9954f999"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data shape:  (400, 2)\n",
            "Number of violence videos in test data:  199\n",
            "Number of non violence videos in test data:  201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save test dataframe in a csv file"
      ],
      "metadata": {
        "id": "4j8UdHb5EJ0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv(rlvs_dataframes_folder + \"test.csv\", index=False)"
      ],
      "metadata": {
        "id": "9ogPAYGND8o-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining some video properties"
      ],
      "metadata": {
        "id": "l3eCv8hT7ao5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_steps = 32   # Number of frames of each video\n",
        "color_channels = 3  # Number of color channels\n",
        "height = 128  # Height of each frame\n",
        "width = 128   # Width of each frame"
      ],
      "metadata": {
        "id": "FZI_eYp9wKPG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to perform the preprocessing of the videos. Videos that contain a greater number of frames than the amount passed to the class will be cut. The videos that contain less than the average amount will be completed with zeros until reaching the average."
      ],
      "metadata": {
        "id": "Jznc7U65F7-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def capture(filename, time_steps, color_channels, height, width):\n",
        "  # Create an array to store the video frames after being processed\n",
        "  frames = np.zeros((time_steps, color_channels, height, width), dtype=float)\n",
        "  # VideoCapture object to open and read the video\n",
        "  video_capture = cv2.VideoCapture(filename)\n",
        "  # To check if the VideoCapture object was able to open the video\n",
        "  if video_capture.isOpened():\n",
        "    # To keep track of how many frames have been stored in the frames array\n",
        "    frames_counter = 0\n",
        "    while frames_counter < time_steps:\n",
        "      # Read the next frame\n",
        "      is_frame_read, frame = video_capture.read()\n",
        "      # Check if there are no more frames available\n",
        "      if not is_frame_read:\n",
        "        break\n",
        "      # Resize the original frame to the specified dimensions (height, width, color_channels) keeping its original aspect ratio\n",
        "      frame = resize(frame, (height, width, color_channels))\n",
        "      # To add an extra dimension (1, height, width, color_channels)\n",
        "      frame = np.expand_dims(frame, axis=0)\n",
        "      # Moves axis -1 (last axis) to index 1 (1, color_channels, height, width)\n",
        "      frame = np.moveaxis(frame, -1, 1)\n",
        "      # Normalization of the pixel values of the frame (if necessary)\n",
        "      if np.max(frame) > 1:\n",
        "        frame = frame / 255.0\n",
        "      # Store the processed frame in the corresponding position within the frames array\n",
        "      frames[frames_counter][:] = frame\n",
        "      frames_counter += 1\n",
        "\n",
        "    del frame\n",
        "    del is_frame_read\n",
        "  frames = np.moveaxis(frames, 1, 0)  # [channels, frames, height, width]\n",
        "\n",
        "  return frames\n",
        "\n",
        "\n",
        "class TaskDataset(Dataset):\n",
        "  def __init__(self, data, time_steps=40, color_channels=3, height=256, width=256):\n",
        "    # data is a pandas dataframe that contains the paths to the video files with their labels\n",
        "    self.data_locations = data\n",
        "    self.time_steps, self.color_channels, self.height, self.width = time_steps, color_channels, height, width\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_locations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    # To process the video and get its frames\n",
        "    video = capture(self.data_locations.iloc[idx, 0], self.time_steps, self.color_channels, self.height, self.width)\n",
        "    # Dictionary containing the processed video, its corresponding label and its path\n",
        "    sample = {\n",
        "      'video': torch.from_numpy(video),\n",
        "      'label': torch.from_numpy(np.asarray(self.data_locations.iloc[idx, 1])),\n",
        "      'path': self.data_locations.iloc[idx, 0]\n",
        "    }\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "75w19Jm-AQ9k"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the training data to the TaskDataset class"
      ],
      "metadata": {
        "id": "Ovilu1997v2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TaskDataset(\n",
        "  data=train_data, time_steps=time_steps, color_channels=color_channels, height=height, width=width\n",
        ")"
      ],
      "metadata": {
        "id": "CeLQOlH07V3v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the test data to the TaskDataset class"
      ],
      "metadata": {
        "id": "D9RVUd2N79qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TaskDataset(\n",
        "  data=test_data, time_steps=time_steps, color_channels=color_channels, height=height, width=width\n",
        ")"
      ],
      "metadata": {
        "id": "gg4SNZEywNnc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the train batch size"
      ],
      "metadata": {
        "id": "XcX1qv758N3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "dLvYOBTe6sPM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a `DataLoader` to load data in batches during training"
      ],
      "metadata": {
        "id": "1oUYJ6bb8VYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "  dataset=train_dataset,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  pin_memory=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0,\n",
        "  shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "PUGGUKj4wrMM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a `DataLoader` to load data in batches during test"
      ],
      "metadata": {
        "id": "wofB1R0Z8ubr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_BATCH_SIZE = 10"
      ],
      "metadata": {
        "id": "aUlN-R7zrqmr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "  dataset=test_dataset,\n",
        "  batch_size=TEST_BATCH_SIZE,\n",
        "  pin_memory=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0,\n",
        "  shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "-b6w6Xma8Hmo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting the `DataLoaders` in the `dataloaders` dictionary and their sizes in the `dataset_sizes` dictionary"
      ],
      "metadata": {
        "id": "mXYn9dDH9_SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {'train': train_loader, 'test': test_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'test': len(test_dataset)}\n",
        "print(dataloaders)\n",
        "print(dataset_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxa06A5q8KU4",
        "outputId": "478bd8d5-50e2-44de-9d95-8a6399fb8aab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': <torch.utils.data.dataloader.DataLoader object at 0x7fb77dc9f2e0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7fb77dc9f100>}\n",
            "{'train': 1600, 'test': 400}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To realease the memory because `data`, `train_data` and `test_data` are no longer needed"
      ],
      "metadata": {
        "id": "yk6RslET-jZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del data\n",
        "del train_data\n",
        "del test_data"
      ],
      "metadata": {
        "id": "m7keMujw1--3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.- ViViT**"
      ],
      "metadata": {
        "id": "LA8k9CNlvcZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "  def __init__(self, dimension, fn):\n",
        "    super(PreNorm, self).__init__()\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    return self.fn(self.norm(x), **kwargs)"
      ],
      "metadata": {
        "id": "PEZKiOBb801C"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, dimension, heads=8, head_dimension=64, dropout=0.):\n",
        "    super(Attention, self).__init__()\n",
        "    inner_dim = head_dimension * heads\n",
        "    project_out = not (heads == 1 and head_dimension == dimension)\n",
        "\n",
        "    self.heads = heads\n",
        "    self.scale = head_dimension ** -0.5\n",
        "\n",
        "    self.attend = nn.Softmax(dim=-1)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.to_qkv = nn.Linear(dimension, inner_dim * 3, bias=False)\n",
        "\n",
        "    self.to_out = nn.Sequential(\n",
        "      nn.Linear(inner_dim, dimension),\n",
        "      nn.Dropout(dropout)\n",
        "    ) if project_out else nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "    q, k, v = map(lambda t: rearrange(\n",
        "      t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "    dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "    attn = self.attend(dots)\n",
        "    attn = self.dropout(attn)\n",
        "\n",
        "    out = torch.matmul(attn, v)\n",
        "    out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "    return self.to_out(out)"
      ],
      "metadata": {
        "id": "a_l4aDxZ86wc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, dimension, hidden_dimension, dropout=0.):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "      nn.Linear(dimension, hidden_dimension),\n",
        "      nn.GELU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(hidden_dimension, dimension),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)"
      ],
      "metadata": {
        "id": "AZv_9UsK8_LT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, dimension, layers, heads, head_dimension, mlp_dimension, dropout=0.):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for _ in range(layers):\n",
        "      self.layers.append(nn.ModuleList([\n",
        "        PreNorm(dimension, Attention(dimension, heads=heads,\n",
        "          head_dimension=head_dimension, dropout=dropout)),\n",
        "        PreNorm(dimension, FeedForward(\n",
        "          dimension, mlp_dimension, dropout=dropout))\n",
        "      ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for attn, ff in self.layers:\n",
        "      x = attn(x) + x\n",
        "      x = ff(x) + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "6hQVG1M-9BmT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViViT(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    height,\n",
        "    width,\n",
        "    frames,\n",
        "    patch_height,\n",
        "    patch_width,\n",
        "    patch_frame,\n",
        "    number_classes,\n",
        "    dimension,\n",
        "    layers=4,\n",
        "    heads=3,\n",
        "    in_channels=3,\n",
        "    head_dimension=64,\n",
        "    dropout=0.,\n",
        "    embedding_dropout=0.,\n",
        "    mlp_dimension=4\n",
        "  ):\n",
        "    super(ViViT, self).__init__()\n",
        "\n",
        "    assert height % patch_height == 0 and width % patch_width == 0, 'Image dimensions must be divisible by the patch size'\n",
        "    assert frames % patch_frame == 0, 'Frames must be divisible by frame patch size'\n",
        "\n",
        "    number_image_patches = (height // patch_height) * \\\n",
        "      (width // patch_width)\n",
        "    number_frame_patches = (frames // patch_frame)\n",
        "\n",
        "    patch_dimension = in_channels * patch_height * patch_width * patch_frame\n",
        "\n",
        "    self.patch_embedding = nn.Sequential(\n",
        "      Rearrange('b c (f pf) (h p1) (w p2) -> b f (h w) (p1 p2 pf c)',\n",
        "        p1=patch_height, p2=patch_width, pf=patch_frame),\n",
        "      nn.LayerNorm(patch_dimension),\n",
        "      nn.Linear(patch_dimension, dimension),\n",
        "      nn.LayerNorm(dimension)\n",
        "    )\n",
        "\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(\n",
        "      1, number_frame_patches, number_image_patches, dimension))\n",
        "    self.dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "    self.spatial_cls_token = nn.Parameter(torch.randn(1, 1, dimension))\n",
        "    self.spatial_transformer = Transformer(\n",
        "      dimension, layers, heads, head_dimension, mlp_dimension, dropout)\n",
        "\n",
        "    self.temporal_cls_token = nn.Parameter(torch.randn(1, 1, dimension))\n",
        "    self.temporal_transformer = Transformer(\n",
        "      dimension, layers, heads, head_dimension, mlp_dimension, dropout)\n",
        "\n",
        "    self.to_latent = nn.Identity()\n",
        "\n",
        "    self.mlp_head = nn.Sequential(\n",
        "      nn.LayerNorm(dimension),\n",
        "      nn.Linear(dimension, number_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embedding(x)\n",
        "    b, f, n, _ = x.shape\n",
        "\n",
        "    x = x + self.pos_embedding[:, :f, :n]\n",
        "\n",
        "    spatial_cls_tokens = repeat(\n",
        "      self.spatial_cls_token, '1 1 d -> b f 1 d', b=b, f=f)\n",
        "    x = torch.cat((spatial_cls_tokens, x), dim=2)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = rearrange(x, 'b f n d -> (b f) n d')\n",
        "\n",
        "    x = self.spatial_transformer(x)\n",
        "    x = rearrange(x, '(b f) n d -> b f n d', b=b)\n",
        "    x = x[:, :, 0]\n",
        "\n",
        "    temporal_cls_tokens = repeat(\n",
        "      self.temporal_cls_token, '1 1 d-> b 1 d', b=b)\n",
        "    x = torch.cat((temporal_cls_tokens, x), dim=1)\n",
        "\n",
        "    x = self.temporal_transformer(x)\n",
        "    x = x[:, 0]\n",
        "\n",
        "    x = self.to_latent(x)\n",
        "\n",
        "    return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "N3x3vPPH9FsJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.- Training**"
      ],
      "metadata": {
        "id": "Wcl6fWQO-swI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, device='cuda', num_epochs=7):\n",
        "  model.to(device)\n",
        "\n",
        "  # Start the training time\n",
        "  since = time.time()\n",
        "\n",
        "  # Save the best loss value during model training\n",
        "  best_loss = float('inf')\n",
        "\n",
        "  # Create a copy of the current model weights\n",
        "  best_model_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions_counter = 0\n",
        "\n",
        "    # To create a progress bar to iterate over the 'train' dataloader using the tqdm library\n",
        "    progress_bar = tqdm(dataloaders['train'], total=int(len(dataloaders['train'])))\n",
        "\n",
        "    for batch, sample in enumerate(progress_bar):\n",
        "      # Get the videos and labels and move them to the corresponding device memory\n",
        "      inputs = sample['video'].to(device, dtype=torch.float)  # [batch_size, time_steps, color_channels, height, width]\n",
        "      labels = sample['label'].view(sample['label'].shape[0], 1).to(device, dtype=torch.float)  # [batch_size] -> [batch_size, 1]\n",
        "\n",
        "      # To clean up the accumulated gradients and ensure that the gradients are calculated correctly \n",
        "      # for the current batch during backpropagation and updating of the weights\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Get the outputs predicted by the model\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Calculate the loss with the function specified in the criterion variable\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Computes the gradients of all model parameters with respect to the loss function\n",
        "      loss.backward()\n",
        "\n",
        "      # Update model parameters based on gradients computed during backpropagation\n",
        "      optimizer.step()\n",
        "\n",
        "      # To get the total loss of the current batch:\n",
        "      #   - loss.item() is the scalar value of the current batch loss\n",
        "      #   - inputs.size(0) gets the batch size\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "      # Apply a sigmoid activation function to the outputs to obtain the predictions\n",
        "      # and round the predictions to be binary (0 or 1)\n",
        "      predictions = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "      # Adds the number of correct predictions in the current batch to the accumulated correct predictions counter\n",
        "      correct_predictions_counter += torch.sum(predictions == labels.data)\n",
        "\n",
        "    # Calculates the average loss for each epoch\n",
        "    epoch_loss = running_loss / dataset_sizes['train']\n",
        "    # Calculates the accuracy for each epoch\n",
        "    epoch_accuracy = correct_predictions_counter.double() / dataset_sizes['train']\n",
        "    print('Train Loss: {:.4f} Accuracy: {:.4f}'.format(epoch_loss, epoch_accuracy))\n",
        "\n",
        "    # Updates the state of the optimizer based on the loss obtained in each training epoch\n",
        "    scheduler.step(epoch_loss)\n",
        "\n",
        "    # Stores the model weights that correspond to the best loss achieved so far\n",
        "    if epoch_loss < best_loss:\n",
        "      best_loss = epoch_loss\n",
        "      best_model_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "  # End the training time\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "  # The model is loaded with the weights corresponding to the best saved model\n",
        "  model.load_state_dict(best_model_weights)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "arZEXHecxGtN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model"
      ],
      "metadata": {
        "id": "N4QmYaLnD3sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViViT(\n",
        "  height=height,\n",
        "  width=width,\n",
        "  frames=time_steps,\n",
        "  patch_height=8,\n",
        "  patch_width=8,\n",
        "  patch_frame=8,\n",
        "  number_classes=1,\n",
        "  dimension=128,\n",
        "  layers=8,\n",
        "  heads=8,\n",
        "  in_channels=3,\n",
        "  head_dimension=64,\n",
        "  dropout=0.,\n",
        "  embedding_dropout=0.,\n",
        "  mlp_dimension=4\n",
        ")\n",
        "model"
      ],
      "metadata": {
        "id": "UdCFypQjxL0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc913d86-6232-4ac7-e373-70a364aae8eb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViViT(\n",
              "  (patch_embedding): Sequential(\n",
              "    (0): Rearrange('b c (f pf) (h p1) (w p2) -> b f (h w) (p1 p2 pf c)', p1=8, p2=8, pf=8)\n",
              "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): Linear(in_features=1536, out_features=128, bias=True)\n",
              "    (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (spatial_transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (network): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=4, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=4, out_features=128, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (temporal_transformer): Transformer(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x ModuleList(\n",
              "        (0): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): Attention(\n",
              "            (attend): Softmax(dim=-1)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
              "            (to_out): Sequential(\n",
              "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (1): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): PreNorm(\n",
              "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (fn): FeedForward(\n",
              "            (network): Sequential(\n",
              "              (0): Linear(in_features=128, out_features=4, bias=True)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Dropout(p=0.0, inplace=False)\n",
              "              (3): Linear(in_features=4, out_features=128, bias=True)\n",
              "              (4): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_latent): Identity()\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "model = train_model(model, criterion, optimizer, scheduler, device=device, num_epochs=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "a82cbb5bb8fc4d39b08553e5b94015a3",
            "589154f59cc54310a75b859dce4d856c",
            "1f34080f1da64a22bde0789c02f09970",
            "1a57300afe284242a7058f2cea0d96ba",
            "4f9a4c82f4614c14900a940aeeb18630",
            "631362d2b5524547863889d1fafdca00",
            "20b4928b637f4d49b7f3908713eea98d",
            "c8c50805d2054128b7031dd4d0822a5b",
            "3d834c0fe29c4b4d88d65f36132ee938",
            "fab4709dbc354ef4bb9f5b596e957383",
            "f55c9ac11bc7486b8e54c38e1bb7acce"
          ]
        },
        "id": "j4dZONWiAzSc",
        "outputId": "6884c7c7-53f6-460b-d4a7-d4a02b6f7289"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "----------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a82cbb5bb8fc4d39b08553e5b94015a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2bbd9f451bb1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-f5a91f48c8ff>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m# Computes the gradients of all model parameters with respect to the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;31m# Update model parameters based on gradients computed during backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.- Test**"
      ],
      "metadata": {
        "id": "GVbyk4L-o5FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, criterion, device='cuda'):\n",
        "  model.to(device)\n",
        "\n",
        "  # To start the evaluation time\n",
        "  since = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  correct_predictions_counter = 0\n",
        "\n",
        "  pred_vs_real = {}\n",
        "  pred_vs_real['path']= []\n",
        "  pred_vs_real['label']= []  \n",
        "  pred_vs_real['prediction']= []\n",
        "\n",
        "  # To create a progress bar to iterate over the 'test' dataloader using the tqdm library\n",
        "  progress_bar = tqdm(dataloaders['test'], total=int(len(dataloaders['test'])))\n",
        "\n",
        "  processed_batch_counter = 0\n",
        "  for batch, sample in enumerate(progress_bar):\n",
        "    # Get the videos and labels and move them to the corresponding device memory\n",
        "    inputs = sample['video'].to(device , dtype=torch.float)\n",
        "    labels = sample['label'].view(sample['label'].shape[0], 1).to(device, dtype=torch.float)\n",
        "    paths = sample['path']\n",
        "\n",
        "    # Get the outputs predicted by the model\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Apply a sigmoid activation function to the outputs to obtain the predictions\n",
        "    # and round the predictions to be binary (0 or 1)\n",
        "    predictions = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "    # Add the predictions and labels to the dictionary pred_vs_real\n",
        "    # converted to a numpy array and move them to CPU memory\n",
        "    pred_vs_real['prediction'].extend(predictions.cpu().detach().numpy().flatten())\n",
        "    pred_vs_real['label'].extend(labels.cpu().detach().numpy().flatten())\n",
        "    pred_vs_real['path'].extend(list(paths))\n",
        "\n",
        "    # Calculate the loss with the function specified in the criterion variable\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # To get the total loss of the current batch:\n",
        "    #   - loss.item() is the scalar value of the current batch loss\n",
        "    #   - inputs.size(0) gets the batch size\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    # Adds the number of correct predictions in the current batch to the accumulated correct predictions counter\n",
        "    correct_predictions_counter += torch.sum(predictions == labels.data)\n",
        "\n",
        "    # Updates the progress message in the progress_bar iterator showing the average loss\n",
        "    # To do this, divide the accumulated loss by the total number of samples processed so far\n",
        "    processed_batch_counter += 1\n",
        "    progress_bar.set_postfix(loss=(running_loss / (processed_batch_counter * dataloaders['test'].batch_size)))\n",
        "\n",
        "  final_loss = running_loss / dataset_sizes['test']\n",
        "  accuracy = correct_predictions_counter.double() / dataset_sizes['test']\n",
        "  precision = precision_score(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  recall = recall_score(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  f1 = f1_score(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  print('{} Loss: {:.4f} Accuracy: {:.4f} Precision: {:.4f} Recall: {:.4f} F1 Score: {:.4f}'.format('Test', final_loss, accuracy, precision, recall, f1))\n",
        "\n",
        "  # Calculate and print the confusion matrix\n",
        "  confusion = confusion_matrix(pred_vs_real['label'], pred_vs_real['prediction'])\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion)\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print('Testing complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "  return pred_vs_real"
      ],
      "metadata": {
        "id": "AF9ldUTwo1nY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_vs_real = test_model(model, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "a1bcee788e434b74b5eb4c1d3201d52f",
            "3188afc930434f86b7b7cd828649ef3f",
            "95589afca35449ed8e5b2c2c4282fa15",
            "6024c01adb6f46a1b6f4721cea92c5b1",
            "e3bbda22a7eb4a6d8a2525680e18b1c9",
            "df803846cf59437db37dde53ee437895",
            "72ed71477027460b8f3048cf1fc4d6fe",
            "f0005fab6a64494a9680415e23134f12",
            "099c7014fcba45948aefb4253e0dda66",
            "a302c9f096bd43e5ae717ce2d3caa37b",
            "ba4c2769480d4b21b31851684fe2a992"
          ]
        },
        "id": "pG38rmi5pV_m",
        "outputId": "54af400a-8286-4ffc-948c-8b8b640b0d51"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1bcee788e434b74b5eb4c1d3201d52f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7360 Accuracy: 0.5025 Precision: 0.0000 Recall: 0.0000 F1 Score: 0.0000\n",
            "Confusion Matrix:\n",
            "[[201   0]\n",
            " [199   0]]\n",
            "Testing complete in 7m 40s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model test results in a CSV file"
      ],
      "metadata": {
        "id": "yRLaxI0BTM7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with the data from pred_vs_real\n",
        "pred_vs_real_dataframe = pd.DataFrame({'path': pred_vs_real['path'], 'label': pred_vs_real['label'], 'prediction': pred_vs_real['prediction']})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "pred_vs_real_dataframe.to_csv(rlvs_dataframes_folder + 'results.csv', index=False)"
      ],
      "metadata": {
        "id": "n5xSYQ60rIWO"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}